{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-8-ops/WEEK-2-PROJECT-MODEL-COMPARISON/blob/main/multiple_linear_regression_for_LuxAcad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CazISR8X_HUG"
      },
      "source": [
        "# Multiple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOyqYHTk_Q57"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5HdAEYhHP7_J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgC61-ah_WIz"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('MOCK_DATA (8).csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "VVQ5TyP5RCOG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0DAy3rVUV5Q",
        "outputId": "424dfd30-5468-45ef-e4c3-54a777893fc7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['New York' 1971]\n",
            " ['Los Angeles' 1316]\n",
            " ['Los Angeles' 2273]\n",
            " ...\n",
            " ['New York' 1702]\n",
            " ['Los Angeles' 1656]\n",
            " ['New York' 1392]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding the Categorical Data\n"
      ],
      "metadata": {
        "id": "hlFjQd8fRIJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "metadata": {
        "id": "Jp564PUiSYq0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emvozuXYUlZJ",
        "outputId": "c3b9025c-1696-413d-a3f8-a233a2dfa09a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 1971]\n",
            " [0.0 1.0 0.0 1316]\n",
            " [0.0 1.0 0.0 2273]\n",
            " ...\n",
            " [0.0 0.0 1.0 1702]\n",
            " [0.0 1.0 0.0 1656]\n",
            " [0.0 0.0 1.0 1392]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WemVnqgeA70k"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "v1pMPTmiSTMV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCM05wp8WJf0",
        "outputId": "3b922b53-0543-490a-d04a-5b7af46cc1ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 1296]\n",
            " [0.0 1.0 0.0 2722]\n",
            " [1.0 0.0 0.0 1081]\n",
            " ...\n",
            " [1.0 0.0 0.0 1782]\n",
            " [0.0 0.0 1.0 2907]\n",
            " [0.0 0.0 1.0 508]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPJqh_yjWST9",
        "outputId": "b00ce9f6-228b-4c67-8ebb-28fd645eb995"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[560.16 826.3  476.99 530.79 823.54 919.88 312.42 631.25 887.43 285.78\n",
            " 539.94 753.35 511.14 418.34 787.08 577.22 808.46 463.79 325.95 138.47\n",
            " 645.02 427.39 957.59 729.94 730.52 497.74 220.76 350.05 620.4  269.97\n",
            " 904.85 380.66 756.58 421.24 544.71 263.27 854.7   99.87 482.03 499.53\n",
            "  58.83 432.13  62.69 993.18 981.25 509.98 797.14 563.59 800.61 811.41\n",
            " 273.68 238.97 260.56 553.8  849.88 143.64 281.27 787.68 572.89 720.35\n",
            " 191.34 594.35 214.11 551.9  419.2  589.36 271.37 645.61 437.44 518.66\n",
            " 859.83 132.72 980.23 120.41 461.65 193.89 760.67 969.04 646.25 863.52\n",
            " 174.94 204.87 635.77 164.38 153.8  407.58 647.92 849.54 267.17 280.25\n",
            " 259.95 966.33 376.43 511.09 549.82 456.98 514.28 425.12 560.52 274.65\n",
            " 656.78 950.08 291.9  203.11 143.42 621.47 342.52 644.61 323.23 622.79\n",
            " 694.2  372.22  64.8  867.91 373.94 532.77 742.84 571.19 897.57 141.06\n",
            " 178.42 870.8  539.97 707.   825.31 639.3  894.22 516.61 879.05 668.74\n",
            " 127.26 478.84 854.32 114.55 416.15 794.92 117.95 560.83 156.59 228.88\n",
            " 520.68 126.48 334.99  93.55 648.22 734.78 759.4  242.21 275.77 578.54\n",
            " 941.66 950.91 958.06 622.03 534.73 859.17 707.59 254.43 142.05 599.86\n",
            " 793.84 685.51 307.41 691.61 983.4  539.36  60.67 770.4  793.03 814.59\n",
            " 738.84  92.64 301.55 729.99 432.04 478.25 597.   998.01 137.66 509.64\n",
            " 548.98 756.72 677.44 948.17 643.95 358.61 176.65 117.62 620.36 140.96\n",
            " 785.69 173.64 626.25 285.41 321.08 995.05 724.6  764.18 794.13 843.34\n",
            " 640.37 828.26 628.33 368.77 421.02 665.4  470.07 926.84 286.01 281.65\n",
            "  84.89 433.66 317.02 878.5  235.17 390.05 474.93 610.12 745.52 259.06\n",
            " 167.07 847.87 797.78 312.08  56.73 788.49 902.63 448.11  99.31 145.05\n",
            " 906.09 379.93 747.44 298.51 511.92 255.18 988.04 583.27 445.44 452.35\n",
            " 812.03 388.33 895.26 628.9  407.94 662.23 167.64 922.94 604.92 544.13\n",
            " 744.52 126.21 369.2  690.16 204.39 912.15 433.62 891.78 885.32 373.49\n",
            " 733.28 859.47 231.24 883.27 779.17 736.74 721.42 890.36 846.08 793.92\n",
            " 124.42 202.68 103.48 689.7  246.77 467.48 916.99 284.25 843.14 810.43\n",
            " 816.83 743.78 981.32 670.45 942.42 174.46 887.85 256.63 649.69 996.86\n",
            " 949.05 800.39 777.92 500.09  80.92 131.38 742.25 357.46 929.63 327.75\n",
            " 114.9  639.77 135.2  126.88 948.47 743.56 612.81 105.84 957.1  850.29\n",
            " 340.26 130.23 250.35 158.17 757.72 260.06 109.2  277.61 657.87 563.97\n",
            " 151.35 232.33 463.22  96.48  90.13 959.34 887.83 731.01 445.07 471.55\n",
            " 110.94 551.73 102.3  523.03 138.17 484.94 721.72  97.39 648.49 367.15\n",
            " 198.8  777.46 588.35 560.94 374.82 235.19 493.51 953.11 950.1  834.35\n",
            " 681.56 569.39 325.63 311.33 179.43 391.81 161.98 150.09 602.94 486.57\n",
            " 654.82 656.95 286.34 108.3  327.45 443.23 430.03 633.75 909.8  442.84\n",
            " 570.13 442.81 884.13 171.93 765.66 143.86 494.81 716.11 867.23 718.11\n",
            " 263.95  77.85 244.53  51.03 780.11 383.01 478.25  95.75 366.17 638.78\n",
            " 356.44 713.55 284.13 338.2  497.34 301.39  51.16 485.14 898.52 155.37\n",
            "  98.23 807.09 819.55 168.55 655.59 320.96 796.29 354.01 662.29 210.08\n",
            " 291.04 235.22 145.69 282.54 663.72 869.65 200.42  65.83 248.39 986.67\n",
            " 946.2  476.17 954.03 957.42 238.49 888.59 191.81 937.33 514.85 169.56\n",
            " 191.08 964.75 252.7  350.38 976.3  129.7  109.91 409.99 444.69 456.95\n",
            " 540.14 563.74 820.52 802.2  155.44 726.6  714.88 601.3  131.24 656.42\n",
            " 575.05 815.72 988.43  87.28 721.06 805.83 282.22 553.56 831.8   60.05\n",
            " 352.34 326.33  67.35 839.28 726.66 241.26 542.   406.32  64.01 388.27\n",
            " 866.91 479.11 683.9  227.77 453.09 349.88 214.82 244.04 709.31 663.88\n",
            " 699.87 373.71 935.25 251.08 362.06 591.39 346.3   94.7  746.9  766.02\n",
            " 533.   375.75 435.09 891.32 823.46 276.22 776.53  96.58  87.85 747.62\n",
            " 425.84 583.66 756.66 464.55 682.58  67.19 760.24 372.87 861.41 656.57\n",
            " 197.34 895.37  69.35 856.36 935.83  54.17 575.9  817.62 879.64 595.54\n",
            " 734.65 792.89 799.74 909.05 679.99 792.81 153.38  92.02 448.6  768.75\n",
            " 738.01 450.24 164.3  891.79 529.22 343.95 716.45 967.43 851.83 738.\n",
            " 409.6  340.26 568.19 398.1  221.23  77.84 853.41 596.79 122.26 116.\n",
            " 594.6  463.85 988.22 229.45 369.22 735.31 216.56 830.48 202.91 562.23\n",
            " 626.67 514.11 403.62 534.5  384.59 897.63 427.25 254.   640.85 371.59\n",
            " 575.37 679.59 861.57 226.26 201.94 304.47 851.09 748.57 391.83 542.28\n",
            "  87.42 157.88 547.8  113.49 539.92 768.48 706.01 332.74 534.03 913.33\n",
            " 251.32  61.11 379.19 154.05 877.97 871.09 705.2  738.   100.26 973.33\n",
            " 243.02 691.15 235.84 740.78  74.67 525.41 389.88 727.73 341.27 909.54\n",
            " 399.34 327.42 932.59 967.54 950.32 477.54 712.37 965.31 484.26 499.36\n",
            " 225.13 328.19 662.97 259.86 816.04 401.6  792.32 762.35 942.92 134.23\n",
            " 403.61  94.98  56.31 774.2  409.11 102.85 284.37 298.55 618.22 109.38\n",
            " 580.05 332.12 246.78 431.04 862.62 107.53 248.96 910.62 545.94 371.17\n",
            " 200.83 104.03 232.75  68.44 940.89 189.58 800.89 317.22 195.75 770.29\n",
            " 445.32 955.56 279.65 430.42 562.46 412.71 859.02  87.93 728.83 238.69\n",
            " 328.39 832.38 726.27 883.67 698.86 522.76 772.09 913.73 768.   883.99\n",
            " 735.54 816.69 690.03 211.56 186.04 358.24 993.5  748.36 572.8  117.67\n",
            " 639.95  61.8  457.23 510.85 500.71 481.45 117.2  398.62 203.05 257.3\n",
            " 862.72 924.41 793.15 188.51 952.06 133.8  217.04 888.18 423.07 904.09\n",
            " 906.42 851.8  873.34 393.24 333.51 922.59 397.01 578.65 646.81 122.1\n",
            " 322.71 306.08 610.03 520.32 331.55 829.83 711.71 813.55 819.38 671.96\n",
            " 129.37 757.77 431.77 267.97 968.41 989.18 995.96  74.08 163.35 348.27\n",
            " 409.82 809.13 415.25 258.31 156.15 352.21 845.46 686.13 158.84 531.16\n",
            " 698.94 128.76 952.87  97.51  73.22 433.04 883.49  83.63 359.23 675.04\n",
            " 730.24 643.58 927.68 800.62 761.44 329.65 993.55 500.81 177.22  79.71\n",
            " 995.16 315.07  73.15 933.45 974.18 197.41  94.32 433.65 233.93 224.2\n",
            " 504.19 434.95 709.7  545.23 479.79 800.98 219.08 379.43 342.61 205.52\n",
            " 170.04 694.09 698.56 812.71 305.72 833.64 140.52 985.04 382.39 384.13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE SCALING"
      ],
      "metadata": {
        "id": "tlfO4pdtWdgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train[:, 1:] = sc.fit_transform(X_train[:, 1:])\n",
        "X_test[:, 1:] = sc.transform(X_test[:, 1:])\n"
      ],
      "metadata": {
        "id": "ZPPgJ4rXWhrm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ0dZHlZWsk5",
        "outputId": "968ab2e0-22ab-42e2-920a-4e531bf12678"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.4128887718599612 -0.7157425057893192 -0.6747419445519529]\n",
            " [0.0 1.4128887718599612 -0.7157425057893192 1.311800871451113]\n",
            " [1.0 -0.7077697975358527 -0.7157425057893192 -0.9742557632340421]\n",
            " ...\n",
            " [1.0 -0.7077697975358527 -0.7157425057893192 0.0022985944224438665]\n",
            " [0.0 -0.7077697975358527 1.397150500230811 1.569522064270585]\n",
            " [0.0 -0.7077697975358527 1.397150500230811 -1.7724949172100282]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-McZVsQBINc"
      },
      "source": [
        "## Training the Multiple Linear Regression model on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "#we build the model\n",
        "regressor = LinearRegression()\n",
        "#Then train the model on the training set\n",
        "regressor.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "nRxW3KgXXop3",
        "outputId": "043dbaf6-3aa1-4f1c-bdbb-7a5abe51ca30"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNkXL1YQBiBT"
      },
      "source": [
        "## Predicting the Test set results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = regressor.predict(X_test)\n",
        "np.set_printoptions(precision=2)\n",
        "print(np.concatenate((y_predict.reshape(len(y_predict),1),y_test.reshape(len(y_test),1)),1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBq9uwdCZ-im",
        "outputId": "7a98fd5b-aa76-45c0-ca00-39bb35228de1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[536.   518.35]\n",
            " [500.5  666.39]\n",
            " [516.   954.74]\n",
            " [501.   425.52]\n",
            " [515.5  700.96]\n",
            " [515.   860.23]\n",
            " [500.5  518.16]\n",
            " [536.75 234.55]\n",
            " [514.5  696.26]\n",
            " [536.75 914.77]\n",
            " [499.75 999.15]\n",
            " [515.5  260.33]\n",
            " [516.5  507.71]\n",
            " [499.75 949.54]\n",
            " [516.   263.81]\n",
            " [536.75 899.08]\n",
            " [536.   209.  ]\n",
            " [501.   463.4 ]\n",
            " [537.5  323.78]\n",
            " [499.5  287.53]\n",
            " [515.   377.74]\n",
            " [516.    89.89]\n",
            " [536.5  730.04]\n",
            " [500.   498.42]\n",
            " [516.   979.25]\n",
            " [537.25 801.3 ]\n",
            " [515.   748.58]\n",
            " [514.5   82.47]\n",
            " [536.25 503.08]\n",
            " [515.   969.33]\n",
            " [515.5  704.08]\n",
            " [500.5  804.89]\n",
            " [500.25 996.85]\n",
            " [515.5  238.18]\n",
            " [500.25 408.81]\n",
            " [515.5  734.67]\n",
            " [516.   624.03]\n",
            " [500.75 143.41]\n",
            " [501.    99.29]\n",
            " [500.   350.95]\n",
            " [500.5  104.04]\n",
            " [536.25 387.87]\n",
            " [515.5  717.38]\n",
            " [499.75 705.29]\n",
            " [501.   974.3 ]\n",
            " [500.   942.42]\n",
            " [499.5  656.99]\n",
            " [500.25 960.47]\n",
            " [536.75 600.62]\n",
            " [516.   799.95]\n",
            " [536.5  194.44]\n",
            " [536.   304.69]\n",
            " [501.   235.6 ]\n",
            " [536.   387.23]\n",
            " [514.5  437.84]\n",
            " [516.   677.89]\n",
            " [536.5  317.81]\n",
            " [516.   156.75]\n",
            " [516.5  933.55]\n",
            " [500.   863.47]\n",
            " [514.5  809.08]\n",
            " [500.25 764.89]\n",
            " [499.5  284.76]\n",
            " [515.   670.9 ]\n",
            " [515.5  696.61]\n",
            " [500.   125.67]\n",
            " [500.5  528.72]\n",
            " [537.   556.11]\n",
            " [501.   381.44]\n",
            " [515.5  336.37]\n",
            " [515.   170.  ]\n",
            " [500.   583.49]\n",
            " [537.25 912.88]\n",
            " [515.5  995.7 ]\n",
            " [500.25 530.11]\n",
            " [537.25 427.41]\n",
            " [499.5  313.34]\n",
            " [516.   310.44]\n",
            " [537.5  295.46]\n",
            " [536.5  911.65]\n",
            " [536.   147.2 ]\n",
            " [516.5  504.5 ]\n",
            " [535.75 969.88]\n",
            " [515.5  911.9 ]\n",
            " [501.   218.73]\n",
            " [500.   861.27]\n",
            " [515.5  990.17]\n",
            " [536.75 443.67]\n",
            " [537.   133.81]\n",
            " [500.75 786.17]\n",
            " [516.5  389.07]\n",
            " [499.75 638.93]\n",
            " [500.25 919.69]\n",
            " [515.   800.6 ]\n",
            " [501.   581.81]\n",
            " [537.   146.92]\n",
            " [516.5  549.06]\n",
            " [500.   135.09]\n",
            " [536.25 501.85]\n",
            " [501.   971.23]\n",
            " [500.5  236.92]\n",
            " [501.   246.64]\n",
            " [537.   115.86]\n",
            " [500.   183.9 ]\n",
            " [515.   429.21]\n",
            " [500.75 538.32]\n",
            " [536.   295.63]\n",
            " [499.5  978.46]\n",
            " [537.   726.77]\n",
            " [536.   478.25]\n",
            " [536.5  186.15]\n",
            " [499.75 479.63]\n",
            " [515.   893.63]\n",
            " [499.75  53.17]\n",
            " [515.5  195.66]\n",
            " [536.   801.04]\n",
            " [516.   431.94]\n",
            " [536.5  762.1 ]\n",
            " [515.5  791.55]\n",
            " [536.75  86.46]\n",
            " [515.   958.72]\n",
            " [516.   325.39]\n",
            " [501.   503.26]\n",
            " [500.75 299.57]\n",
            " [536.   389.35]\n",
            " [501.   424.5 ]\n",
            " [515.5  686.55]\n",
            " [500.75 622.82]\n",
            " [500.5  238.89]\n",
            " [499.5  601.15]\n",
            " [536.75 916.88]\n",
            " [500.   872.76]\n",
            " [516.   717.18]\n",
            " [500.   413.29]\n",
            " [499.75 296.28]\n",
            " [499.75 114.81]\n",
            " [537.25 212.24]\n",
            " [536.75 513.64]\n",
            " [515.5  622.81]\n",
            " [515.   419.95]\n",
            " [514.5  690.95]\n",
            " [501.   881.07]\n",
            " [500.75 194.09]\n",
            " [537.25 423.54]\n",
            " [516.5  571.03]\n",
            " [536.75 853.82]\n",
            " [515.5   91.69]\n",
            " [500.75 385.27]\n",
            " [500.25 910.06]\n",
            " [501.    59.2 ]\n",
            " [500.   893.01]\n",
            " [537.   747.44]\n",
            " [515.   968.29]\n",
            " [515.5  374.42]\n",
            " [516.   230.33]\n",
            " [515.   412.65]\n",
            " [515.   282.4 ]\n",
            " [537.   431.06]\n",
            " [500.75 201.4 ]\n",
            " [516.   873.65]\n",
            " [536.25 105.92]\n",
            " [536.25 821.18]\n",
            " [516.   167.96]\n",
            " [536.   403.58]\n",
            " [537.   745.18]\n",
            " [515.5  523.38]\n",
            " [499.5  433.13]\n",
            " [515.   214.2 ]\n",
            " [500.   125.48]\n",
            " [499.5  882.82]\n",
            " [514.5  984.04]\n",
            " [536.75 347.02]\n",
            " [500.25 179.95]\n",
            " [537.75 828.17]\n",
            " [537.5  784.33]\n",
            " [536.25 376.95]\n",
            " [501.25 127.56]\n",
            " [500.25 785.63]\n",
            " [536.25 215.47]\n",
            " [500.   892.04]\n",
            " [499.75 412.13]\n",
            " [536.5  984.86]\n",
            " [515.5  171.36]\n",
            " [536.   548.42]\n",
            " [536.   496.65]\n",
            " [500.   943.22]\n",
            " [500.   706.28]\n",
            " [536.75 909.12]\n",
            " [499.75 348.37]\n",
            " [536.75 776.22]\n",
            " [536.25 685.38]\n",
            " [537.   887.36]\n",
            " [515.5  304.68]\n",
            " [499.75 444.12]\n",
            " [500.   202.56]\n",
            " [516.    76.6 ]\n",
            " [516.   194.15]\n",
            " [537.   930.31]\n",
            " [501.   959.6 ]\n",
            " [537.    58.48]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, y_predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvPBNR8o7Nl1",
        "outputId": "20a4aa32-991c-4769-e2d9-8ebfa62d9848"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.005964138993396428"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}